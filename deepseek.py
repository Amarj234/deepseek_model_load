# -*- coding: utf-8 -*-
"""deepseek.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zkpxllkT8WO1R0bW8WfZgHdtjcWLHRbB
"""

from transformers import pipeline

# Create the pipeline
pipe = pipeline("text-generation", model="deepseek-ai/DeepSeek-R1", trust_remote_code=True)

# Prompt as plain string, not list
prompt = "Who are you?"

# Generate text
output = pipe(prompt, max_new_tokens=100)
print(output[0]["generated_text"])

!pip install --upgrade transformers accelerate

!pip install torch -q
from huggingface_hub import login

# Paste your Hugging Face token (from https://huggingface.co/settings/tokens)
#login(token="your_hf_token")

# Commented out IPython magic to ensure Python compatibility.
# %env HF_TOKEN=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

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "deepseek-ai/deepseek-llm-7b-chat"

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# Chat-style prompt
prompt = "<|system|>\nYou are a helpful assistant.\n<|user|>\nWho are you?\n<|assistant|>\n"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

prompt = "<|system|>\nYou are a helpful assistant.\n<|user|>\ncan you write a code for flutter login page?\n<|assistant|>\n"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=1000)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import pipeline

# Create the text generation pipeline using DeepSeek-R1
pipe = pipeline("text-generation", model="deepseek-ai/DeepSeek-Prover-V2-7B", trust_remote_code=True)

# Define your prompt
prompt = "Who are you?"

# Generate response
output = pipe(prompt, max_new_tokens=100)

# Print the generated text
print(output[0]["generated_text"])